# Wav2Lip从入门到入土

**Wav2Lip** 是一个基于深度学习的开源项目，主要用于**生成与音频同步的唇形视频**。简单来说，它可以让视频中的人物嘴唇动作与输入的语音完美匹配，常用于修复影视配音口型不同步、制作虚拟人物对话视频等场景。

> ***“Generating accurate lip-sync by learning from a well-trained lip-sync expert."***
> 

**GitHub**:[GitHub - Rudrabha/Wav2Lip: This repository contains the codes of "A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild", published at ACM Multimedia 2020. For HD commercial model, please try out Sync Labs](https://github.com/Rudrabha/Wav2Lip)

---

# Section 1. 论文阅读

**论文链接：**[A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild](https://arxiv.org/abs/2008.10010)
看不懂英文可以上传到豆包，可以左边解释右边译文，非常的方便。

## 1.1  模型背景

**随着视听内容消费的快速增长，视频内容创作及多语言化需求愈发迫切，唇同步技术成为关键。**

> With the exponential rise in the consumption of audio-visual content [21], rapid video content creation has become a quintessential need. At the same time, making these videos accessible in different languages is also a key challenge. For instance, a deep learning lecture series, a famous movie, or a public address to the nation, if translated to desired target languages, can become accessible to millions of new viewers. A crucial aspect of translating such talking face videos or creating new ones is correcting the lip sync to match the desired target speech. Consequently, lip-syncing talking face videos to match a given input audio stream has received considerable attention [6, 13, 17, 18, 23] in the research community.
> 

**然而，现有方法在处理动态、非受限视频时存在严重不足，难以精准匹配任意身份的唇部动作与新音频，Wav2Lip 模型旨在解决这一难题。**

> We start by inspecting the existing speaker-independent approaches for speech to lip generation. We find that these models do not adequately penalize wrong lip shapes, either as a result of using only reconstruction losses or weak lip-sync discriminators. We adapt a powerful lip-sync discriminator that can enforce the generator to consistently produce accurate, realistic lip motion. Next, we re-examine the current evaluation protocols and devise new, rigorous evaluation benchmarks derived from three standard test sets. We also propose reliable evaluation metrics using SyncNet [9] to precisely evaluate lip sync in unconstrained videos. We also collect and release ReSyncED, a challenging set of real-world videos that can benchmark how the models will perform in practice. We conduct extensive quantitative and subjective human evaluations and outperform previous methods by a large margin across all benchmarks. Our key contributions/claims are as follows:
> 

## 1.2 模型架构

### 1.2.1 生成器

采用与 **LipGAN** 类似的生成器架构，通过让生成器生成 5 帧连续图像，由专家判别器计算 “专家同步损失” 来惩罚不准确的生成，训练时判别器权重固定。

- 身份编码器：一堆残差卷积层，将随机参考帧 R 和姿态先验 P（下半部分掩码的目标脸）沿通道轴连接后进行编码。
- 语音编码器：通过 2D 卷积对输入语音段 S 进行编码，然后与面部表示连接。
- 人脸解码器：由卷积层和转置卷积层组成，负责上采样生成最终的面部图像，生成器的目标是最小化生成帧与真实帧之间的 L1 重建损失。

### 1.2.2 判别器

基于 **SyncNet** 进行改进。**SyncNet** 输入连续的视频帧（仅下半部分）和语音段，通过计算面部编码器和音频编码器生成的嵌入之间的 L2 距离，并使用最大间隔损失进行训练，以判别音频和视频是否同步。

- 提供彩色图像
- 模型更深，有残差跳跃连接
- 采用余弦相似性与二进制交叉熵损失作为损失函数

### 1.2.3 提高人脸质量

强大的判别器有时迫使生成器产生准确的唇形。但有时会导致变形区域略微模糊或包含轻微的伪影，为了减轻这种轻微的质量损失，训练一个视觉质量判别器，与生成器在 **GAN** 框架下训练。整个网络通过两个判别器优化同步精度和视觉质量。

## 1.3 训练过程

### 1.3.1 **损失函数**

**综合使用多种损失函数来优化模型。**

- **L1 重建损失**用于确保生成的面部图像在整体上与真实图像相似，计算生成帧与真实帧之间的 L1 距离并求平均；
- **专家同步损失**基于专家判别器，通过最小化判别器输出的同步概率的负对数来实现，促使生成器生成更准确的唇同步；
- **对抗损失**由视觉质量判别器产生，视觉质量判别器在 GAN 设置下训练，用于惩罚不真实的面部生成，提高生成图像的视觉质量。

**最终的总损失函数是这三种损失的加权和.**

在实验中，同步惩罚权重和对抗损失权重分别经验性地设置为 0.03 和 0.07。

### 1.3.2 **训练设置**

- 模型仅在 **LRS2** 训练集上进行训练，批量大小设置为 80。
- 使用 **Adam** 优化器，初始学习率为 1e-4，β1 = 0.5，β2 = 0.999。
- 在训练过程中，专家唇同步判别器的权重保持不变，仅对生成器和视觉质量判别器的参数进行更新。

## 1.4 模型评估

重新审视现有评估框架，发现其存在不反映真实使用情况、评估不一致、无法检查时间一致性以及现有指标不针对唇同步等问题。

为此，提出新的评估指标 **LSE-D**（越低表示音频 - 视频匹配越好）和 **LSE-C**（越高表示音频 - 视频相关性越好），基于 **SyncNet** 计算得到。

创建了基于 LRS2、LRW 和 LRS3 测试集的新基准测试集，通过随机采样不同视频的音频与测试视频配对，形成一致的测试集。

收集并发布 **ReSyncED** 数据集，包含不同类型的真实世界视频，如配音视频、随机配对视频和文本转语音合成视频，进行定量评估和人类评估。

## 1.5 应用场景

Wav2Lip 在多个领域具有广泛的应用潜力。

在视频翻译方面，可将英语讲座视频的唇部动作与其他语言的配音同步，提升多语言学习体验；

在电影配音中，校正唇同步，增强观影感受；

在会议翻译中，解决现场翻译时演讲者嘴唇与翻译语音不同步的问题；

在动画制作中，为 CGI 角色自动添加准确的唇同步动画，节省大量人工时间。

---

# Section 2. **官方Demo测试**

[Google Colab](https://colab.research.google.com/drive/1o3f-uwICOR51ty--RBhjrK7seU_gP9vY?usp=sharing)

用 **常熟阿诺**直播时候的痞帅**正脸**和黄胶鞋七分裤山羊胡子**刘海柱**的**声音。**

[result_voice_1.mp4](result_voice_1.mp4)

---

# Section 3. 部分代码拆解

## 3.1  **数据预处理流程**（视频+音频处理）

### 3.1.1 **解析命令行参数**

```python
# 使用argparse模块创建了一个命令行参数解析器。
parser = argparse.ArgumentParser()

# 参数--ngpu; 用于并行运行的 GPU 数量，默认值为 1，类型为整数。
parser.add_argument('--ngpu', help='Number of GPUs across which to run in parallel', default=1, type=int)
# 参数--batch_size; 单个 GPU 进行人脸检测的批处理大小，默认值为 32，类型为整数。
parser.add_argument('--batch_size', help='Single GPU Face detection batch size', default=32, type=int)
# 参数--data_root; LRS2 数据集的根文件夹，这个参数是必需的。
parser.add_argument("--data_root", help="Root folder of the LRS2 dataset", required=True)
#参数--preprocessed_root; 预处理后的数据集的根文件夹，这个参数也是必需的。
parser.add_argument("--preprocessed_root", help="Root folder of the preprocessed dataset", required=True)

# 解析命令行参数，并将结果存储在args对象中。
args = parser.parse_args()
```

### 3.1.2 **初始化人脸检测模型**

根据指定的 GPU 数量创建一个包含面部对齐对象的列表，每个对象都针对特定的 GPU 设备，并设置了二维面部标志类型和不翻转输入的参数。

```python
fa = [face_detection.FaceAlignment(
						# 二维的面部标志类型
						face_detection.LandmarksType._2D, 
						# 不翻转输入
						flip_input=False,  
						# 使用 CUDA 设备，设备编号由变量 id 确定
            device='cuda:{}'.format(id)
				) for id in range(args.ngpu)]
```

### 3.1.3 **定义 FFmpeg 命令模板**

 定义一个 `FFmpeg` 命令模板，用于后续的音频提取。

```python
template = 'ffmpeg -loglevel panic -y -i {} -strict -2 {}'
```

### 3.1.4 用于处理视频的函数

```python
def process_video_file(vfile, args, gpu_id):
		# 使用 cv2.VideoCapture 打开视频文件
    video_stream = cv2.VideoCapture(vfile)
    
    # 逐帧读取视频，并将帧存储在 frames 列表中
    frames = []
    while 1:
        still_reading, frame = video_stream.read()
        if not still_reading:
            video_stream.release()
            break
        frames.append(frame)
    
    # 提取视频文件名和所在目录名
    vidname = os.path.basename(vfile).split('.')[0]
    dirname = vfile.split('/')[-2]

		# 创建保存预处理结果的目录
    fulldir = path.join(args.preprocessed_root, dirname, vidname)
    os.makedirs(fulldir, exist_ok=True)

		# 将帧按批处理大小进行分组
    batches = [frames[i:i + args.batch_size] for i in range(0, len(frames), args.batch_size)]

		# 对每个批次的帧进行人脸检测，将检测到的人脸裁剪并保存为 JPEG 图像。
    i = -1
    for fb in batches:
        preds = fa[gpu_id].get_detections_for_batch(np.asarray(fb))

        for j, f in enumerate(preds):
            i += 1
            if f is None:
                continue

            x1, y1, x2, y2 = f
            cv2.imwrite(path.join(fulldir, '{}.jpg'.format(i)), fb[j][y1:y2, x1:x2])
```

## **3.2 模型架构总览**（生成器&判别器）

### 3.2.1 生成器

`Wav2Lip` 类主要用于生成任务，将音频和面部信息结合生成输出。

```python
class Wav2Lip(nn.Module):
    def __init__(self):
        super(Wav2Lip, self).__init__()

				# 身份编码器（参考帧编码）
        self.face_encoder_blocks = nn.ModuleList([
            nn.Sequential(Conv2d(6, 16, kernel_size=7, stride=1, padding=3)), # 96,96

            nn.Sequential(Conv2d(16, 32, kernel_size=3, stride=2, padding=1), # 48,48
            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),
            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True)),

            nn.Sequential(Conv2d(32, 64, kernel_size=3, stride=2, padding=1),    # 24,24
            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),
            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),
            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True)),

            nn.Sequential(Conv2d(64, 128, kernel_size=3, stride=2, padding=1),   # 12,12
            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),
            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True)),

            nn.Sequential(Conv2d(128, 256, kernel_size=3, stride=2, padding=1),       # 6,6
            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),
            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True)),

            nn.Sequential(Conv2d(256, 512, kernel_size=3, stride=2, padding=1),     # 3,3
            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),),
            
            nn.Sequential(Conv2d(512, 512, kernel_size=3, stride=1, padding=0),     # 1, 1
            Conv2d(512, 512, kernel_size=1, stride=1, padding=0)),])

				# 语音编码器（音频特征提取）
        self.audio_encoder = nn.Sequential(
            Conv2d(1, 32, kernel_size=3, stride=1, padding=1),
            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),
            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),

            Conv2d(32, 64, kernel_size=3, stride=(3, 1), padding=1),
            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),
            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),

            Conv2d(64, 128, kernel_size=3, stride=3, padding=1),
            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),
            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),

            Conv2d(128, 256, kernel_size=3, stride=(3, 2), padding=1),
            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),

            Conv2d(256, 512, kernel_size=3, stride=1, padding=0),
            Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)

				# 人脸解码器（生成面部动画）
        self.face_decoder_blocks = nn.ModuleList([
            nn.Sequential(Conv2d(512, 512, kernel_size=1, stride=1, padding=0),),

            nn.Sequential(Conv2dTranspose(1024, 512, kernel_size=3, stride=1, padding=0), # 3,3
            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),),

            nn.Sequential(Conv2dTranspose(1024, 512, kernel_size=3, stride=2, padding=1, output_padding=1),
            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),
            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),), # 6, 6

            nn.Sequential(Conv2dTranspose(768, 384, kernel_size=3, stride=2, padding=1, output_padding=1),
            Conv2d(384, 384, kernel_size=3, stride=1, padding=1, residual=True),
            Conv2d(384, 384, kernel_size=3, stride=1, padding=1, residual=True),), # 12, 12

            nn.Sequential(Conv2dTranspose(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),
            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),
            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),), # 24, 24

            nn.Sequential(Conv2dTranspose(320, 128, kernel_size=3, stride=2, padding=1, output_padding=1), 
            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),
            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),), # 48, 48

            nn.Sequential(Conv2dTranspose(160, 64, kernel_size=3, stride=2, padding=1, output_padding=1),
            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),
            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),),]) # 96,96

				# 输出层
        self.output_block = nn.Sequential(Conv2d(80, 32, kernel_size=3, stride=1, padding=1),
            nn.Conv2d(32, 3, kernel_size=1, stride=1, padding=0),
            nn.Sigmoid()) 

    def forward(self, audio_sequences, face_sequences):
				# audio_sequences：音频序列数据；face_sequences：面部序列数据
        # audio_sequences = (B, T, 1, 80, 16)，其中B是批次大小，T是时间步长
        
        B = audio_sequences.size(0) # 音频序列数据的批次大小。
        input_dim_size = len(face_sequences.size()) # 面部序列数据的维度数量。
        if input_dim_size > 4: # 如果input_dim_size大于 4，对音频序列和面部序列数据进行处理
            audio_sequences = torch.cat([audio_sequences[:, i] for i in range(audio_sequences.size(1))], dim=0)
            face_sequences = torch.cat([face_sequences[:, :, i] for i in range(face_sequences.size(2))], dim=0)
				
				# 音频编码
        audio_embedding = self.audio_encoder(audio_sequences) # B, 512, 1, 1
				
				# 面部编码
        feats = []
        x = face_sequences
        # 遍历将面部序列数据x依次通过每个模块进行编码，并将编码结果添加到feats列表中。
        for f in self.face_encoder_blocks: 
            x = f(x)
            feats.append(x)

				# 面部解码
        x = audio_embedding
        # 遍历这些模块，将音频嵌入x依次通过每个模块进行解码。
        for f in self.face_decoder_blocks:
            x = f(x)
            try:
                x = torch.cat((x, feats[-1]), dim=1)
            except Exception as e:
                print(x.size())
                print(feats[-1].size())
                raise e
            
            feats.pop()

        x = self.output_block(x)

        if input_dim_size > 4:
            x = torch.split(x, B, dim=0) # [(B, C, H, W)]
            outputs = torch.stack(x, dim=2) # (B, C, T, H, W)

        else:
            outputs = x
            
        return outputs
```

### 3.2.2 判别器

- **同步判别器（SyncNet）**
    
    目标是判断生成的唇形动作是否与输入音频同步。该模块通过对比音频特征和视频帧的时间序列差异来优化生成器。
    
    ```python
    class SyncNet_color(nn.Module):
        def __init__(self):
            super(SyncNet_color, self).__init__()
    				
    				# 面部编码器 用于对输入的面部序列数据进行特征提取
            self.face_encoder = nn.Sequential(
                Conv2d(15, 32, kernel_size=(7, 7), stride=1, padding=3),
    
                Conv2d(32, 64, kernel_size=5, stride=(1, 2), padding=1),
                Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),
                Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),
    
                Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
                Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),
                Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),
                Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),
    
                Conv2d(128, 256, kernel_size=3, stride=2, padding=1),
                Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),
                Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),
    
                Conv2d(256, 512, kernel_size=3, stride=2, padding=1),
                Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),
                Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),
    
                Conv2d(512, 512, kernel_size=3, stride=2, padding=1),
                Conv2d(512, 512, kernel_size=3, stride=1, padding=0),
                Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)
    				
    				# 音频编码器 对输入的音频序列数据进行特征提取
            self.audio_encoder = nn.Sequential(
                Conv2d(1, 32, kernel_size=3, stride=1, padding=1),
                Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),
                Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),
    
                Conv2d(32, 64, kernel_size=3, stride=(3, 1), padding=1),
                Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),
                Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),
    
                Conv2d(64, 128, kernel_size=3, stride=3, padding=1),
                Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),
                Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),
    
                Conv2d(128, 256, kernel_size=3, stride=(3, 2), padding=1),
                Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),
                Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),
    
                Conv2d(256, 512, kernel_size=3, stride=1, padding=0),
                Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)
    
        def forward(self, audio_sequences, face_sequences): # audio_sequences := (B, dim, T)
            face_embedding = self.face_encoder(face_sequences)
            audio_embedding = self.audio_encoder(audio_sequences)
    
            audio_embedding = audio_embedding.view(audio_embedding.size(0), -1)
            face_embedding = face_embedding.view(face_embedding.size(0), -1)
    
            audio_embedding = F.normalize(audio_embedding, p=2, dim=1)
            face_embedding = F.normalize(face_embedding, p=2, dim=1)
    
            return audio_embedding, face_embedding
    
    ```
    
- **视觉质量判别器（GAN Discriminator）**
    
    `Wav2Lip_disc_qual` 类主要用于鉴别任务，判断输入的面部序列的真实性。
    
    用于提升生成画面的细节真实感。它通过对抗训练区分生成帧与真实帧的差异。
    
    ```python
    class Wav2Lip_disc_qual(nn.Module):
        def __init__(self):
            super(Wav2Lip_disc_qual, self).__init__()
    				
    				# 面部编码器
            self.face_encoder_blocks = nn.ModuleList([
                nn.Sequential(nonorm_Conv2d(3, 32, kernel_size=7, stride=1, padding=3)), # 48,96
    
                nn.Sequential(nonorm_Conv2d(32, 64, kernel_size=5, stride=(1, 2), padding=2), # 48,48
                nonorm_Conv2d(64, 64, kernel_size=5, stride=1, padding=2)),
    
                nn.Sequential(nonorm_Conv2d(64, 128, kernel_size=5, stride=2, padding=2),    # 24,24
                nonorm_Conv2d(128, 128, kernel_size=5, stride=1, padding=2)),
    
                nn.Sequential(nonorm_Conv2d(128, 256, kernel_size=5, stride=2, padding=2),   # 12,12
                nonorm_Conv2d(256, 256, kernel_size=5, stride=1, padding=2)),
    
                nn.Sequential(nonorm_Conv2d(256, 512, kernel_size=3, stride=2, padding=1),       # 6,6
                nonorm_Conv2d(512, 512, kernel_size=3, stride=1, padding=1)),
    
                nn.Sequential(nonorm_Conv2d(512, 512, kernel_size=3, stride=2, padding=1),     # 3,3
                nonorm_Conv2d(512, 512, kernel_size=3, stride=1, padding=1),),
                
                nn.Sequential(nonorm_Conv2d(512, 512, kernel_size=3, stride=1, padding=0),     # 1, 1
                nonorm_Conv2d(512, 512, kernel_size=1, stride=1, padding=0)),])
    				
    				# 二元预测模块
            self.binary_pred = nn.Sequential(nn.Conv2d(512, 1, kernel_size=1, stride=1, padding=0), nn.Sigmoid())
            # 标签噪声参数
            self.label_noise = .0
    
    		# 用于获取输入面部序列的下半部分
        def get_lower_half(self, face_sequences):
            return face_sequences[:, :, face_sequences.size(2)//2:]
    	
    		# 将输入的面部序列转换为二维张量
        def to_2d(self, face_sequences):
            B = face_sequences.size(0)
            face_sequences = torch.cat([face_sequences[:, :, i] for i in range(face_sequences.size(2))], dim=0)
            return face_sequences
    
    		# 计算对抗损失
        def perceptual_forward(self, false_face_sequences):
    		    # 对输入的虚假面部序列进行处理
            false_face_sequences = self.to_2d(false_face_sequences)
            false_face_sequences = self.get_lower_half(false_face_sequences)
    
            false_feats = false_face_sequences
            for f in self.face_encoder_blocks:
                false_feats = f(false_feats)
    				# 使用二元交叉熵损失函数F.binary_cross_entropy计算预测结果与全为 1 的标签之间的损失，返回该损失值。
            false_pred_loss = F.binary_cross_entropy(self.binary_pred(false_feats).view(len(false_feats), -1), 
                                            torch.ones((len(false_feats), 1)).cuda())
    
            return false_pred_loss
    
    		# 前向传播
        def forward(self, face_sequences):
    			  # 对输入的面部序列进行处理
            face_sequences = self.to_2d(face_sequences)
            face_sequences = self.get_lower_half(face_sequences)
    
            x = face_sequences
            for f in self.face_encoder_blocks:
                x = f(x)
    
            return self.binary_pred(x).view(len(x), -1)
    ```
    

## **3.3 特征融合机制**（跨模态对齐核心）

跨模态特征融合的核心机制体现在**生成器的前向传播部分：**

```python
def forward(self, audio_sequences, face_sequences): #来
				'''
				略
				'''
				# 面部解码
        **x = audio_embedding**
        # 遍历这些模块，将音频嵌入x依次通过每个模块进行解码。
        **for f in self.face_decoder_blocks:
            x = f(x)
            try:
                x = torch.cat((x, feats[-1]), dim=1)
            except Exception as e:
                print(x.size())
                print(feats[-1].size())
                raise e
            feats.pop()**
				'''
				略
				'''
```

## **3.4 损失函数设计**（多目标优化）

### 3.4.1 **L1 重建损失（L1 Reconstruction Loss）**

```python
l1loss = recon_loss(g, gt)  # g=生成图像, gt=真实图像
```

### 3.4.2 **专家同步损失（Expert Sync Loss）**

利用预训练的 `SyncNet` 模型计算音频-唇形同步性。

```python
# 二元交叉熵损失函数
logloss = nn.BCELoss()
# 余弦损失函数    a和v是两个输入张量，用于计算余弦相似度，y是真实标签张量。
def cosine_loss(a, v, y):
    d = nn.functional.cosine_similarity(a, v)
    loss = logloss(d.unsqueeze(1), y)

    return loss
# **同步损失函数**    mel是梅尔频谱特征张量，g是输入的图像特征张量。
def get_sync_loss(mel, g):
    g = g[:, :, :, g.size(3)//2:]
    g = torch.cat([g[:, :, i] for i in range(syncnet_T)], dim=1)
    # B, 3 * T, H//2, W
    a, v = syncnet(mel, g)
    y = torch.ones(g.size(0), 1).float().to(device)
    return cosine_loss(a, v, y)

# 在训练中调用：
sync_loss = get_sync_loss(mel, g)
```

### 3.4.3 **对抗损失（Adversarial Loss）**

生成器试图最大化判别器对生成图像的判定为真的概率。

```python
# 计算**对抗损失**(来自**视觉质量判别器 Wav2Lip_disc_qual**)
def perceptual_forward(self, false_face_sequences):
    # 对输入的虚假面部序列进行处理
    false_face_sequences = self.to_2d(false_face_sequences)
    false_face_sequences = self.get_lower_half(false_face_sequences)

    false_feats = false_face_sequences
    for f in self.face_encoder_blocks:
        false_feats = f(false_feats)
		# 使用二元交叉熵损失函数F.binary_cross_entropy计算预测结果与全为 1 的标签之间的损失，返回该损失值。
    false_pred_loss = F.binary_cross_entropy(
			   self.binary_pred(false_feats).view(len(false_feats), -1), 
         torch.ones((len(false_feats), 1)).cuda()
         )

    return false_pred_loss

# 生成器训练时：
perceptual_loss = disc.perceptual_forward(g)  # 对抗损失

# 判别器训练时：
pred_real = disc(gt)       # 真实图像判定为真
pred_fake = disc(g.detach()) # 生成图像判定为假
disc_real_loss = BCE(pred_real, 1)
disc_fake_loss = BCE(pred_fake, 0)
```

## **3.5 训练策略**（两阶段训练技巧）

### **3.5.1 训练专家判别器（可以直接下载预训练的权重）**

使用color_syncnet_train.py单独训练`SyncNet`，使其能够准确判断音频与唇形的同步性。

```python
# 数据生成逻辑（随机选择正/负样本）
if random.choice([True, False]):
    y = torch.ones(1).float()  # 正样本
    chosen = img_name
else:
    y = torch.zeros(1).float() # 负样本
    chosen = wrong_img_name

# 模型前向与损失计算
a, v = model(mel, x)  # SyncNet 输出音频和视频特征
loss = cosine_loss(a, v, y)
```

### 3.5.2 **训练 Wav2Lip 模型**

- **预加载 SyncNet**
    
    ```python
    # 加载预训练 SyncNet 并冻结参数
    syncnet = SyncNet().to(device)
    load_checkpoint(args.syncnet_checkpoint_path, syncnet, None, reset_optimizer=True)
    for p in syncnet.parameters():
        p.requires_grad = False
    ```
    
- **联合训练生成器与视觉质量判别器**
    
    ```python
    # 生成器损失混合
    loss = hparams.syncnet_wt * sync_loss + \
           hparams.disc_wt * perceptual_loss + \
           (1. - hparams.syncnet_wt - hparams.disc_wt) * l1loss
    
    # 动态调整同步损失权重 （当 average_sync_loss < 0.75 时设为 0.03）。
    if average_sync_loss < .75:
        hparams.set_hparam('syncnet_wt', 0.03)
    ```
    
- **交替优化策略**
    - **生成器优化**：先计算生成器损失并反向传播。
    - **判别器优化**：固定生成器，更新判别器参数。
    
    ```python
    # 生成器优化
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # 判别器优化
    disc_optimizer.zero_grad()
    disc_real_loss = BCE(disc(gt), 1)  # 真实帧判定为真
    disc_fake_loss = BCE(disc(g.detach()), 0)  # 生成帧判定为假
    (disc_real_loss + disc_fake_loss).backward()
    disc_optimizer.step()
    ```
    

## **3.6 推理优化**（后处理与加速）

- 通过 `--wav2lip_batch_size` 控制推理批次大小（默认 128），提升 GPU 利用率。
    
    ```python
    
    **parser.add_argument('--wav2lip_batch_size', type=int, help='Batch size for Wav2Lip model(s)', default=128)**
    
    def datagen(frames, mels):
    		'''
    		略
    		'''
    		if len(img_batch) >= **args.wav2lip_batch_size:**
    			img_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)
    
    			img_masked = img_batch.copy()
    			img_masked[:, args.img_size//2:] = 0
    
    			img_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.
    			mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])
    
    			**yield img_batch, mel_batch, frame_batch, coords_batch**
    			img_batch, mel_batch, frame_batch, coords_batch = [], [], [], []
    
    		'''
    		略
    		'''
    ```
    
- **人脸检测框平滑**
    
    ```python
    **parser.add_argument('--nosmooth', default=False, action='store_true',
    					help='Prevent smoothing face detections over a short temporal window')**
    
    def face_detect(images):
    	'''
    	略
    	'''
    	if not **args.nosmooth**: boxes = get_smoothened_boxes(boxes, T=5)
    	'''
    	略
    	'''
    ```
    
- **唇形区域融合优化**
    
    ```python
    # 生成结果仅替换下半脸
    p = cv2.resize(p.astype(np.uint8), (x2 - x1, y2 - y1))
    f[y1:y2, x1:x2] = p  # 局部替换
    ```
    
- **FFmpeg 硬件编码加速**
    
    ```python
    command = 'ffmpeg -y -i {} -i {} -strict -2 -q:v 1 {}'.format(args.audio, 'temp/result.avi', args.outfile)
    subprocess.call(command, shell=platform.system() != 'Windows')
    ```
    

---

# Section 4. 数字人唇音同步练习

[Google Colab](https://colab.research.google.com/drive/14ZedAIhUxcpC62x-vNpXaSOWQbICvo2z?usp=sharing)

[result_voice.mp4](result_voice.mp4)

---

# Section 5. **相关技术**

## 5.1 AD-NeRF

[AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis](https://arxiv.org/abs/2103.11078)

[GitHub - YudongGuo/AD-NeRF: This repository contains a PyTorch implementation of "AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis".](https://github.com/YudongGuo/AD-NeRF)

## 5.2 **MeshTalk**

[MeshTalk: 3D Face Animation from Speech using Cross-Modality...](https://arxiv.org/abs/2104.08223)

[GitHub - facebookresearch/meshtalk: Code for MeshTalk: 3D Face Animation from Speech using Cross-Modality Disentanglement](https://github.com/facebookresearch/meshtalk)

---

# Section 6. 应用：**流式推理 + 推流**

## 6.1 **基础概念解析**

### 6.1.1 **流式推理（Streaming Inference）**

所谓**流式推理**，就是将输入数据（如音频、视频）分割为连续的小块（如每 200ms），逐块实时处理，类似于视频直播的“边录边传”，而非录制完整视频再上传。

所以他的应用场景也就是**实时**，比如虚拟主播、AI新闻播报。

### 6.1.2 **推流（Streaming）**

既然要“上传”“处理”，那么就要将音视频数据通过网络传输协议发送到服务器或其他接收端，这就是**推流**。

## 6.2 **实现流程**

**数据输入       →       缓冲       →       Wav2Lip 流式推理      →     视频编码     →      推流**
   (实时)             (低延迟生成)                    (H.264/RTMP)

**流式推理+推流**的整个流程，我们可以想象一个小机器人（其实就是AI模型），它眼睛看着当前捕获的人脸照片，听着此时的声音，预测嘴巴应该怎么动并生成新的口型。

### 6.2.1 **音视频流捕获**

在实时交互场景中，音频和视频需要以流式的方式输入系统。

音频的流式输入意味着系统持续接收音频数据块，而不是一次性加载整个音频文件。那么如何“流”入呢，通过麦克风实时采集音频，按固定时间窗口分帧（例如每 100ms 一帧）。

而视频的流入同理，通过摄像头采集面部图像，按相同时间窗口分帧。

### 6.2.2 **动态缓冲区管理**

实时流有不确定性。比如网络抖动、硬件性能波动可能导致音视频数据到达时间不稳定；

因而需要**实现音视频数据的缓存与对齐，支持流式分块处理：**

- **环形缓冲区（Ring Buffer）**
    - **内存高效**：自动覆盖旧数据，避免内存溢出（适合长时间运行）。
    - **低延迟**：仅保留最近 N 秒数据，减少处理延迟。
- **动态调整策略**：
    - 当检测到处理延迟增加时，自动缩小缓冲区（牺牲稳定性换低延迟）。
    - 当系统空闲时，扩大缓冲区（提高稳定性应对突发负载）。

### **6.2.3 流式推理优化**

- **分块推理**：将长音频/视频分割为重叠的短片段（如 1 秒长度，重叠 0.2 秒），送入模型推理。
- **模型轻量化**：使用 TensorRT 或 ONNX 加速推理，或对原始模型进行剪枝/量化（如 FP16 精度）。
- **异步处理**：音频处理和视频生成在不同线程中并行，通过队列传递数据。

### 6.2.4 **音视频同步与补偿**

- **时间戳对齐**：为每一帧音频和视频标记精确的时间戳（PTS, Presentation Timestamp）。
- **动态延迟补偿**：若视频处理速度慢于音频，丢弃过时的视频帧或插值生成中间帧。

### **6.2.5 实时推流**

- 使用 `FFmpeg`将处理后的视频帧编码为 RTMP/HLS 流，推送至直播服务器（如 OBS、Bilibili 直播等）。

## 6.3 **关键技术挑战与解决方案**

### **6.3.1 低延迟与高吞吐的平衡**

- **问题**：模型推理耗时高，难以满足实时性。
- **解决方案**：
    - **模型优化**
    - **动态批处理（Dynamic Batching）**：在 GPU 空闲时合并多个请求的推理任务。
    - **分辨率调整**：降低输入视频分辨率（如 256x256 → 128x128），牺牲部分质量换取速度。

### 6.3.2 **音视频同步抖动**

- **问题**：音频和视频处理速度不一致导致唇形不同步。
- **解决方案**：
    - **自适应缓冲区**：根据当前处理延迟动态调整缓冲区大小。
    - **丢帧策略**：若视频延迟超过阈值（如 200ms），强制丢弃旧帧，用最新帧覆盖。

### 6.3.3 **流式推流的稳定性**

- **问题**：网络波动导致推流卡顿或中断。
- **解决方案**：
    - **重传机制**：对关键帧（I-Frame）设置重传次数限制。
    - **带宽适应**：根据网络状态动态调整视频码率（如从 2000kbps 降至 1000kbps）。

---

# 参考

1. 知乎：**《Talking-Face-Generation系列之Wav2Lip模型》**

[](https://zhuanlan.zhihu.com/p/613996840)

1. YouTube：**Deepfake Speech & Synchronize Lip Movements with Wav2Lip!**

[Deepfake Speech & Synchronize Lip Movements with Wav2Lip!](https://www.youtube.com/watch?v=FkfEvzUlVJk)
